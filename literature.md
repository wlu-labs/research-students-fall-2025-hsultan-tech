# Literature Review: Vision-and-Language Navigation (VLN)

## Survey Papers


Paper 1: https://arxiv.org/pdf/2203.12667
22 March 2022

| Paper | Problem Addressed | Key Methodologies | Main Findings & Contributions | Limitations & Open Questions | Relevance |
|-------|------------------|-------------------|-------------------------------|------------------------------|-----------|
| Gu et al. (2022) – *Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions* | Overview of VLN tasks and methods for building agents that understand natural language instructions and navigate 3D environments. | Structured review of tasks, datasets, evaluation metrics, and methods (representation learning, action strategy learning, data-centric learning, prior exploration). Categorizes benchmarks by communication complexity and task objectives. | (1) Benchmarks taxonomy classifying VLN datasets by communication complexity (initial instruction, oracle guidance, dialogue) and task objectives (fine-grained/coarse-grained navigation, object interaction). (2) Four-component hierarchy of approaches: representation learning, action strategy learning, data-oriented learning, and prior exploration. (3) Evaluation framework with metrics like SR, SPL, and nDTW/SDTW. (4) Identified open issues: generalization to unseen environments, simulation-to-reality transfer, dataset diversity, multi-agent cooperation, and ethics/privacy. | Current limitations include poor generalization to unseen environments, limited dataset diversity (mainly American homes), and lack of real-world deployment. Future directions focus on multi-agent VLN, simulation-to-reality transfer, and multicultural datasets. | Foundational survey providing comprehensive overview of VLN field, useful for identifying research gaps and future directions in AI and robotics. |


Paper 2: https://aclanthology.org/2024.findings-emnlp.269.pdf

| Paper | Problem Addressed | Key Methodologies | Main Findings & Contributions | Limitations & Open Questions | Relevance |
|-------|------------------|-------------------|-------------------------------|------------------------------|-----------|
| Wang et al. (2024) – *Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation* | Evaluates whether existing VLN models truly understand fine-grained navigation instructions. Argues that current metrics (e.g., success rate, SPL) overestimate performance. | Introduced **NAVNUANCES dataset** covering 5 atomic instruction categories (direction change, vertical movement, landmark recognition, region recognition, numerical comprehension). Built using context-free grammar (CFG) + LLM-assisted generation + human refinement. Benchmarked supervised models (Seq2Seq, CLIP-ViL, VLN-BERT, HAMT, DUET, BEVBERT, ScaleVLN) and zero-shot LLM-enhanced agents (NavGPT, NavGPT4, NavGPT4v). | (1) Proposed a **new evaluation framework** that diagnoses VLN models at atomic-instruction level. (2) Created **NAVNUANCES dataset** with 1,700+ curated instances across 90 environments. (3) Comprehensive benchmarking revealed stagnation in **numerical comprehension**, selective directional biases, and inconsistent spatial reasoning. (4) Demonstrated that LMM-enhanced zero-shot agents outperform supervised ones on landmark recognition and turning but still lag behind humans. | Dataset limited to **static Matterport3D environments** (no object reconfiguration). Focus only on **atomic-level instructions**, not long-sequence reasoning. Numerical comprehension limited to region-level (no object-level counting). Future work should expand to dynamic environments, richer spatial reasoning, and fully automatic dataset generation. | Advances evaluation of VLN by moving beyond coarse metrics. Directly relevant for diagnosing weaknesses of state-of-the-art models and designing agents that generalize better to real-world navigation tasks. |


Paper 3: https://arxiv.org/abs/2210.05714

| Paper | Problem Addressed | Key Methodologies | Main Findings & Contributions | Limitations & Open Questions | Relevance |
|-------|------------------|-------------------|-------------------------------|------------------------------|-----------|
| Huang et al. (2023) – *Visual Language Maps for Robot Navigation* | Existing VLM-based navigation systems can match images to object goals but lack spatial precision and integration with maps. This limits their ability to follow spatial instructions (e.g., “between the sofa and TV”) and to generalize across robots. | Proposed **VLMaps**, a spatial map representation that fuses pretrained visual-language features (LSeg, CLIP) into 3D reconstructions of environments. Combined with LLMs for parsing instructions into code-executable navigation primitives. Evaluated in Habitat, AI2THOR, and real robots. | (1) Introduced **VLMaps**, enabling natural language indexing directly within spatial maps. (2) Demonstrated **zero-shot spatial goal navigation**, handling queries like “3m to the right of the chair.” (3) Enabled **cross-embodiment navigation**, generating custom obstacle maps for different robots (e.g., LoCoBot vs drone). (4) Achieved higher success rates and SPL than LM-Nav, CoW, and CLIP-Map baselines on multi-object and spatial navigation tasks. (5) Validated on real robots, showing ability to follow novel spatial instructions. | Sensitive to odometry drift and noisy 3D reconstructions; struggles with cluttered scenes and visually similar objects (e.g., sofa vs chair). Limited evaluation in dynamic environments; real-world success rate still below simulation. Future work: improve robustness, dynamic scenes, better fusion methods. | Advances VLN by unifying **semantic grounding** (via VLMs) and **spatial precision** (via mapping). Directly relevant to developing agents that can follow natural language with spatial context in real-world environments. |

Paper 4: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10777561

| Paper | Problem Addressed | Key Methodologies | Main Findings & Contributions | Limitations & Open Questions | Relevance |
|-------|------------------|-------------------|-------------------------------|------------------------------|-----------|
| Wang et al. (2025) – *Boosting Efficient Reinforcement Learning for Vision-and-Language Navigation With Open-Sourced LLM* | Existing VLN methods rely heavily on imitation learning (RNNs → poor generalization, Transformers → too large for deployment). Pure RL can adapt but struggles with sparse rewards and long-term instruction reasoning. | Proposed **DILLM-VLN** (Decomposing Instructions with LLMs for VLN): (1) Uses lightweight, open-sourced LLM (ChatGLM-6B) to decompose complex instructions into short, interpretable sub-instructions. (2) **Cascaded Multi-scale Attention (CMA)** to integrate local + global textual context. (3) **Multi-modal Fusion Discriminator (MFD)** to check sub-instruction completion via scene, object, and action signals. RL policy trained with A2C on decomposed subtasks. | (1) Introduced DILLM-VLN, showing LLM-based instruction decomposition improves interpretability and RL efficiency. (2) Achieved **+69.6% SR vs A2C** and **+12.8% vs PPO** baselines on R2R unseen test set. (3) Reduced training parameters by ~50% compared to hierarchical RL methods (DISH), while improving SR by 4.5%. (4) Demonstrated CMA crucial for contextual grounding, MFD critical for flexible switching, via ablation studies. (5) Showed lightweight LLM (ChatGLM-6B) matches GPT-3.5/LLaMA performance but with lower cost + local deployability. | Still trails SOTA transformer VLN models in raw SR despite efficiency gains. Sensitive to LLM segmentation quality (though robust to LLM choice). Experiments limited to R2R dataset; real-world deployment unexplored. Future work: extend to dynamic/continuous environments, continuous action spaces, and real robots. | Highly relevant: combines **open-source LLMs with RL** for VLN, providing interpretability and efficiency. Aligns directly with your research interest in making VLN agents practical for real-world robotics. |



Paper 5: https://arxiv.org/abs/2505.03181
May 6th 2025

| Paper | Problem Addressed | Key Methodologies | Main Findings & Contributions | Limitations & Open Questions | Relevance |
|-------|------------------|-------------------|-------------------------------|------------------------------|-----------|
| Grigsby et al. (2025) – *VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making* | Open-weight VLMs struggle as agents because they fail strict action syntax requirements and cannot generalize over long interactions. Standard supervised fine-tuning (SFT) needs high-quality demonstrations and cannot outperform its dataset. | Proposed **Advantage-Filtered Supervised Fine-Tuning (AFSFT)**, an off-policy RL update: (1) Convert multi-turn agent interactions into token-level RL states. (2) Train a critic head (Qψ) alongside the VLM’s language head (Lϕ). (3) Filter out suboptimal action tokens using advantage estimates. (4) Supports both offline (from logs) and online (interactive) fine-tuning. Tested with MoonDream2, PaliGemma, and xGen-MM VLMs across Gym Cards, BabyAI, and BrowserGym. | (1) Introduced **AFSFT**, bridging supervised fine-tuning and off-policy RL for VLMs. (2) Showed that token-level filtering enables VLM agents to **outperform their demonstrations**. (3) Unified **offline-to-online fine-tuning**, allowing smooth improvement as agents collect new data. (4) Improved both **syntax validity** (producing correct “[action] …” outputs) and **task success rates**, recovering competitive policies even from noisy datasets. (5) Demonstrated scalability to complex multi-modal environments like BrowserGym. | Still requires reliable Q-value estimation; critic miscalibration in large vocabularies can destabilize learning. Success dependent on good action parsing (syntax bottleneck). Limited to simulation tasks (toy card games, BabyAI, MiniWoB); real robotics and continuous control not tested. Future work: stronger critics, chain-of-thought integration, deployment in real-world agent tasks. | Directly relevant: shows how to **align open VLMs with decision-making** without needing perfect expert data. Connects to your interest in RL for VLN/robotics by offering a practical, lightweight RL fine-tuning method. |